import jieba
import math
import re
from collections import Counter

#txt预处理
corpus=[]
with open('data.txt', "r", encoding="ANSI") as file:
    text = [line.strip("\n").replace("\u3000", "").replace("\t", "") for line in file][3:]
    corpus += text

with open('cn_stopwords.txt',"r", encoding="ANSI") as file:#停词读入
    words= [line.strip() for line in file.readlines()]


sub= u'[a-zA-Z0-9’!"#$%&\'()*+,-./:：;「<=>?@，。?★、…【】《》？“”‘’！[\\]^_`{|}~]+'#去除标点
for j in range(len(corpus)):
    corpus[j] = re.sub(sub, "", corpus[j])
    
new_corpus = []
character_count = 0
for text in corpus:
    new_words = []
    split_words = list(jieba.cut(text))
    for word in split_words:
        if word not in words:
            new_words.append(word)
    character_count += len(''.join(map(str, new_words)))
    new_corpus.append(''.join(map(str, new_words)))
    
token = []
for para in corpus:
    token += jieba.lcut(para)
token_num = len(token)
ct = Counter(token)
vocab1 = ct.most_common()
entropy_1gram = sum([-(eve[1]/token_num)*math.log((eve[1]/token_num),2) for eve in vocab1])

